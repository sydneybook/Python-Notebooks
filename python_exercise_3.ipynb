{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "python exercise 3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UJAGPDaIhiKy",
        "outputId": "1f87defd-120d-4df8-c921-95b94277c3a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import pandas as pd\n",
        "import pandas\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error \n",
        "from sklearn.linear_model import LassoLarsCV\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p7na5he9i2jg",
        "outputId": "3d7fa6bc-d4cf-44fb-9853-35a0af318fe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "source": [
        "csvfile = 'drive/My Drive/Colab Notebooks/finalmaster-ratios.csv'\n",
        "alldata = pd.read_csv(csvfile)\n",
        "alldata.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th># Purchases</th>\n",
              "      <th>B01001001</th>\n",
              "      <th>B01001002</th>\n",
              "      <th>B01001003</th>\n",
              "      <th>B01001004</th>\n",
              "      <th>B01001005</th>\n",
              "      <th>B01001006</th>\n",
              "      <th>B01001007</th>\n",
              "      <th>B01001008</th>\n",
              "      <th>B01001009</th>\n",
              "      <th>B01001010</th>\n",
              "      <th>B01001011</th>\n",
              "      <th>B01001012</th>\n",
              "      <th>B01001013</th>\n",
              "      <th>B01001014</th>\n",
              "      <th>B01001015</th>\n",
              "      <th>B01001016</th>\n",
              "      <th>B01001017</th>\n",
              "      <th>B01001018</th>\n",
              "      <th>B01001019</th>\n",
              "      <th>B01001020</th>\n",
              "      <th>B01001021</th>\n",
              "      <th>B01001022</th>\n",
              "      <th>B01001023</th>\n",
              "      <th>B01001024</th>\n",
              "      <th>B01001025</th>\n",
              "      <th>B01001026</th>\n",
              "      <th>B01001027</th>\n",
              "      <th>B01001028</th>\n",
              "      <th>B01001029</th>\n",
              "      <th>B01001030</th>\n",
              "      <th>B01001031</th>\n",
              "      <th>B01001032</th>\n",
              "      <th>B01001033</th>\n",
              "      <th>B01001034</th>\n",
              "      <th>B01001035</th>\n",
              "      <th>B01001036</th>\n",
              "      <th>B01001037</th>\n",
              "      <th>B01001038</th>\n",
              "      <th>B01001039</th>\n",
              "      <th>...</th>\n",
              "      <th>B15002013</th>\n",
              "      <th>B15002014</th>\n",
              "      <th>B15002015</th>\n",
              "      <th>B15002016</th>\n",
              "      <th>B15002017</th>\n",
              "      <th>B15002018</th>\n",
              "      <th>B15002019</th>\n",
              "      <th>B15002020</th>\n",
              "      <th>B15002021</th>\n",
              "      <th>B15002022</th>\n",
              "      <th>B15002023</th>\n",
              "      <th>B15002024</th>\n",
              "      <th>B15002025</th>\n",
              "      <th>B15002026</th>\n",
              "      <th>B15002027</th>\n",
              "      <th>B15002028</th>\n",
              "      <th>B15002029</th>\n",
              "      <th>B15002030</th>\n",
              "      <th>B15002031</th>\n",
              "      <th>B15002032</th>\n",
              "      <th>B15002033</th>\n",
              "      <th>B15002034</th>\n",
              "      <th>B15002035</th>\n",
              "      <th>B19001001</th>\n",
              "      <th>B19001002</th>\n",
              "      <th>B19001003</th>\n",
              "      <th>B19001004</th>\n",
              "      <th>B19001005</th>\n",
              "      <th>B19001006</th>\n",
              "      <th>B19001007</th>\n",
              "      <th>B19001008</th>\n",
              "      <th>B19001009</th>\n",
              "      <th>B19001010</th>\n",
              "      <th>B19001011</th>\n",
              "      <th>B19001012</th>\n",
              "      <th>B19001013</th>\n",
              "      <th>B19001014</th>\n",
              "      <th>B19001015</th>\n",
              "      <th>B19001016</th>\n",
              "      <th>B19001017</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22</td>\n",
              "      <td>206252</td>\n",
              "      <td>469.226965</td>\n",
              "      <td>31.432422</td>\n",
              "      <td>35.219052</td>\n",
              "      <td>33.628765</td>\n",
              "      <td>20.121017</td>\n",
              "      <td>12.610787</td>\n",
              "      <td>6.734480</td>\n",
              "      <td>6.225394</td>\n",
              "      <td>19.432539</td>\n",
              "      <td>28.101546</td>\n",
              "      <td>28.421543</td>\n",
              "      <td>26.390047</td>\n",
              "      <td>31.989993</td>\n",
              "      <td>31.359696</td>\n",
              "      <td>32.116052</td>\n",
              "      <td>32.213021</td>\n",
              "      <td>12.184124</td>\n",
              "      <td>18.361034</td>\n",
              "      <td>9.454454</td>\n",
              "      <td>15.175610</td>\n",
              "      <td>16.281054</td>\n",
              "      <td>11.025348</td>\n",
              "      <td>6.230243</td>\n",
              "      <td>4.518744</td>\n",
              "      <td>530.773035</td>\n",
              "      <td>31.999690</td>\n",
              "      <td>34.322091</td>\n",
              "      <td>32.649380</td>\n",
              "      <td>20.101623</td>\n",
              "      <td>12.513818</td>\n",
              "      <td>8.072649</td>\n",
              "      <td>6.021760</td>\n",
              "      <td>22.923414</td>\n",
              "      <td>31.335454</td>\n",
              "      <td>31.558482</td>\n",
              "      <td>31.063941</td>\n",
              "      <td>36.082074</td>\n",
              "      <td>34.845723</td>\n",
              "      <td>...</td>\n",
              "      <td>64.610300</td>\n",
              "      <td>31.449746</td>\n",
              "      <td>58.735313</td>\n",
              "      <td>20.071053</td>\n",
              "      <td>6.726751</td>\n",
              "      <td>5.882267</td>\n",
              "      <td>543.803963</td>\n",
              "      <td>6.974272</td>\n",
              "      <td>2.504332</td>\n",
              "      <td>5.904107</td>\n",
              "      <td>11.917415</td>\n",
              "      <td>10.767170</td>\n",
              "      <td>18.141844</td>\n",
              "      <td>19.779852</td>\n",
              "      <td>10.956451</td>\n",
              "      <td>181.418442</td>\n",
              "      <td>26.717724</td>\n",
              "      <td>85.271036</td>\n",
              "      <td>54.243532</td>\n",
              "      <td>72.647457</td>\n",
              "      <td>30.816383</td>\n",
              "      <td>2.831933</td>\n",
              "      <td>2.912014</td>\n",
              "      <td>1000</td>\n",
              "      <td>105.667996</td>\n",
              "      <td>82.298375</td>\n",
              "      <td>68.141163</td>\n",
              "      <td>67.336195</td>\n",
              "      <td>63.566902</td>\n",
              "      <td>59.439845</td>\n",
              "      <td>49.409690</td>\n",
              "      <td>53.306757</td>\n",
              "      <td>42.318307</td>\n",
              "      <td>83.167229</td>\n",
              "      <td>89.249208</td>\n",
              "      <td>102.141470</td>\n",
              "      <td>52.872330</td>\n",
              "      <td>36.440765</td>\n",
              "      <td>23.446284</td>\n",
              "      <td>21.197485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>61399</td>\n",
              "      <td>486.538869</td>\n",
              "      <td>22.899396</td>\n",
              "      <td>21.531295</td>\n",
              "      <td>27.036271</td>\n",
              "      <td>16.808091</td>\n",
              "      <td>28.355511</td>\n",
              "      <td>18.192479</td>\n",
              "      <td>13.534422</td>\n",
              "      <td>21.466148</td>\n",
              "      <td>24.886399</td>\n",
              "      <td>23.534585</td>\n",
              "      <td>21.319565</td>\n",
              "      <td>27.101419</td>\n",
              "      <td>30.961416</td>\n",
              "      <td>37.117868</td>\n",
              "      <td>36.466392</td>\n",
              "      <td>12.557208</td>\n",
              "      <td>20.554081</td>\n",
              "      <td>12.182609</td>\n",
              "      <td>15.651721</td>\n",
              "      <td>20.668089</td>\n",
              "      <td>15.961172</td>\n",
              "      <td>10.423623</td>\n",
              "      <td>7.329110</td>\n",
              "      <td>513.461131</td>\n",
              "      <td>18.974250</td>\n",
              "      <td>23.404290</td>\n",
              "      <td>23.892897</td>\n",
              "      <td>17.036108</td>\n",
              "      <td>35.310021</td>\n",
              "      <td>18.534504</td>\n",
              "      <td>17.101256</td>\n",
              "      <td>22.785387</td>\n",
              "      <td>22.150198</td>\n",
              "      <td>22.622518</td>\n",
              "      <td>21.303279</td>\n",
              "      <td>26.971123</td>\n",
              "      <td>32.329517</td>\n",
              "      <td>...</td>\n",
              "      <td>56.929829</td>\n",
              "      <td>46.381727</td>\n",
              "      <td>65.707446</td>\n",
              "      <td>35.509451</td>\n",
              "      <td>16.782205</td>\n",
              "      <td>9.201536</td>\n",
              "      <td>515.086529</td>\n",
              "      <td>3.017306</td>\n",
              "      <td>1.047329</td>\n",
              "      <td>1.371503</td>\n",
              "      <td>6.358785</td>\n",
              "      <td>4.937410</td>\n",
              "      <td>8.303825</td>\n",
              "      <td>9.700264</td>\n",
              "      <td>7.555733</td>\n",
              "      <td>174.155902</td>\n",
              "      <td>25.834123</td>\n",
              "      <td>60.146626</td>\n",
              "      <td>62.440776</td>\n",
              "      <td>76.604658</td>\n",
              "      <td>55.383771</td>\n",
              "      <td>8.977108</td>\n",
              "      <td>9.251409</td>\n",
              "      <td>1000</td>\n",
              "      <td>71.289558</td>\n",
              "      <td>59.062447</td>\n",
              "      <td>54.704688</td>\n",
              "      <td>60.966323</td>\n",
              "      <td>53.012354</td>\n",
              "      <td>60.881706</td>\n",
              "      <td>59.231680</td>\n",
              "      <td>50.093078</td>\n",
              "      <td>40.700626</td>\n",
              "      <td>92.612963</td>\n",
              "      <td>117.363344</td>\n",
              "      <td>113.344051</td>\n",
              "      <td>75.774243</td>\n",
              "      <td>33.000508</td>\n",
              "      <td>33.169741</td>\n",
              "      <td>24.792689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>73170</td>\n",
              "      <td>489.859232</td>\n",
              "      <td>28.905289</td>\n",
              "      <td>36.271696</td>\n",
              "      <td>28.235616</td>\n",
              "      <td>21.566216</td>\n",
              "      <td>12.218122</td>\n",
              "      <td>7.243406</td>\n",
              "      <td>7.380074</td>\n",
              "      <td>16.933169</td>\n",
              "      <td>24.914582</td>\n",
              "      <td>26.896269</td>\n",
              "      <td>31.802651</td>\n",
              "      <td>30.531639</td>\n",
              "      <td>36.258029</td>\n",
              "      <td>35.998360</td>\n",
              "      <td>33.429001</td>\n",
              "      <td>13.625803</td>\n",
              "      <td>19.406861</td>\n",
              "      <td>12.245456</td>\n",
              "      <td>14.664480</td>\n",
              "      <td>21.169878</td>\n",
              "      <td>15.293153</td>\n",
              "      <td>8.610086</td>\n",
              "      <td>6.259396</td>\n",
              "      <td>510.140768</td>\n",
              "      <td>26.171928</td>\n",
              "      <td>30.681973</td>\n",
              "      <td>31.925653</td>\n",
              "      <td>19.789531</td>\n",
              "      <td>10.072434</td>\n",
              "      <td>5.056717</td>\n",
              "      <td>6.218396</td>\n",
              "      <td>15.757824</td>\n",
              "      <td>24.449911</td>\n",
              "      <td>26.595599</td>\n",
              "      <td>27.210605</td>\n",
              "      <td>37.556376</td>\n",
              "      <td>37.050704</td>\n",
              "      <td>...</td>\n",
              "      <td>54.602613</td>\n",
              "      <td>40.613027</td>\n",
              "      <td>43.363788</td>\n",
              "      <td>12.280185</td>\n",
              "      <td>5.796247</td>\n",
              "      <td>3.438452</td>\n",
              "      <td>523.980745</td>\n",
              "      <td>5.422930</td>\n",
              "      <td>4.224384</td>\n",
              "      <td>11.828274</td>\n",
              "      <td>18.331860</td>\n",
              "      <td>15.089891</td>\n",
              "      <td>21.731015</td>\n",
              "      <td>18.685529</td>\n",
              "      <td>7.014441</td>\n",
              "      <td>155.241183</td>\n",
              "      <td>45.466156</td>\n",
              "      <td>71.185775</td>\n",
              "      <td>65.802142</td>\n",
              "      <td>56.272718</td>\n",
              "      <td>24.580018</td>\n",
              "      <td>1.689753</td>\n",
              "      <td>1.414677</td>\n",
              "      <td>1000</td>\n",
              "      <td>102.538696</td>\n",
              "      <td>82.960331</td>\n",
              "      <td>74.828305</td>\n",
              "      <td>79.133495</td>\n",
              "      <td>66.081252</td>\n",
              "      <td>78.245122</td>\n",
              "      <td>63.996993</td>\n",
              "      <td>47.322923</td>\n",
              "      <td>42.505211</td>\n",
              "      <td>70.420610</td>\n",
              "      <td>90.033143</td>\n",
              "      <td>98.677692</td>\n",
              "      <td>54.703249</td>\n",
              "      <td>20.125056</td>\n",
              "      <td>11.890525</td>\n",
              "      <td>16.537397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>94</td>\n",
              "      <td>251724</td>\n",
              "      <td>505.585483</td>\n",
              "      <td>32.054949</td>\n",
              "      <td>31.757004</td>\n",
              "      <td>28.102207</td>\n",
              "      <td>18.651380</td>\n",
              "      <td>12.080692</td>\n",
              "      <td>7.035483</td>\n",
              "      <td>7.686991</td>\n",
              "      <td>25.790151</td>\n",
              "      <td>42.129475</td>\n",
              "      <td>35.824951</td>\n",
              "      <td>32.058922</td>\n",
              "      <td>27.677138</td>\n",
              "      <td>33.842621</td>\n",
              "      <td>38.176733</td>\n",
              "      <td>32.722347</td>\n",
              "      <td>12.493842</td>\n",
              "      <td>16.394940</td>\n",
              "      <td>11.504664</td>\n",
              "      <td>15.914255</td>\n",
              "      <td>16.394940</td>\n",
              "      <td>13.196994</td>\n",
              "      <td>8.648361</td>\n",
              "      <td>5.446441</td>\n",
              "      <td>494.414517</td>\n",
              "      <td>33.123580</td>\n",
              "      <td>28.082344</td>\n",
              "      <td>30.171934</td>\n",
              "      <td>16.863708</td>\n",
              "      <td>9.280005</td>\n",
              "      <td>5.390825</td>\n",
              "      <td>5.609318</td>\n",
              "      <td>19.453846</td>\n",
              "      <td>35.614403</td>\n",
              "      <td>32.082757</td>\n",
              "      <td>28.809331</td>\n",
              "      <td>27.911522</td>\n",
              "      <td>32.690566</td>\n",
              "      <td>...</td>\n",
              "      <td>88.227492</td>\n",
              "      <td>44.076261</td>\n",
              "      <td>87.939148</td>\n",
              "      <td>44.404973</td>\n",
              "      <td>9.671057</td>\n",
              "      <td>7.283569</td>\n",
              "      <td>502.912274</td>\n",
              "      <td>4.509700</td>\n",
              "      <td>0.980370</td>\n",
              "      <td>3.552398</td>\n",
              "      <td>5.986021</td>\n",
              "      <td>7.398907</td>\n",
              "      <td>9.740260</td>\n",
              "      <td>10.605292</td>\n",
              "      <td>7.485410</td>\n",
              "      <td>141.242417</td>\n",
              "      <td>43.078591</td>\n",
              "      <td>84.479020</td>\n",
              "      <td>52.069156</td>\n",
              "      <td>89.836451</td>\n",
              "      <td>33.932320</td>\n",
              "      <td>4.129086</td>\n",
              "      <td>3.886877</td>\n",
              "      <td>1000</td>\n",
              "      <td>61.632139</td>\n",
              "      <td>46.526521</td>\n",
              "      <td>48.437595</td>\n",
              "      <td>54.221644</td>\n",
              "      <td>51.680322</td>\n",
              "      <td>60.066684</td>\n",
              "      <td>54.790900</td>\n",
              "      <td>48.681562</td>\n",
              "      <td>43.873381</td>\n",
              "      <td>84.717507</td>\n",
              "      <td>112.204444</td>\n",
              "      <td>127.137252</td>\n",
              "      <td>83.019904</td>\n",
              "      <td>43.731067</td>\n",
              "      <td>38.851729</td>\n",
              "      <td>40.427349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>37382</td>\n",
              "      <td>495.586111</td>\n",
              "      <td>25.413301</td>\n",
              "      <td>29.318924</td>\n",
              "      <td>26.162324</td>\n",
              "      <td>19.260607</td>\n",
              "      <td>12.893906</td>\n",
              "      <td>6.580707</td>\n",
              "      <td>7.062222</td>\n",
              "      <td>17.334546</td>\n",
              "      <td>32.930287</td>\n",
              "      <td>28.302392</td>\n",
              "      <td>28.569900</td>\n",
              "      <td>26.804344</td>\n",
              "      <td>30.549462</td>\n",
              "      <td>36.595153</td>\n",
              "      <td>42.373335</td>\n",
              "      <td>16.398267</td>\n",
              "      <td>22.871970</td>\n",
              "      <td>17.174041</td>\n",
              "      <td>15.221229</td>\n",
              "      <td>23.433738</td>\n",
              "      <td>14.391953</td>\n",
              "      <td>7.383233</td>\n",
              "      <td>8.560270</td>\n",
              "      <td>504.413889</td>\n",
              "      <td>26.563587</td>\n",
              "      <td>30.255203</td>\n",
              "      <td>24.798031</td>\n",
              "      <td>16.237761</td>\n",
              "      <td>11.101600</td>\n",
              "      <td>4.788401</td>\n",
              "      <td>5.189663</td>\n",
              "      <td>17.842812</td>\n",
              "      <td>30.014445</td>\n",
              "      <td>27.767375</td>\n",
              "      <td>30.763469</td>\n",
              "      <td>25.199294</td>\n",
              "      <td>29.613183</td>\n",
              "      <td>...</td>\n",
              "      <td>102.957039</td>\n",
              "      <td>36.711921</td>\n",
              "      <td>70.039055</td>\n",
              "      <td>33.587502</td>\n",
              "      <td>5.021387</td>\n",
              "      <td>5.244560</td>\n",
              "      <td>511.177236</td>\n",
              "      <td>2.045750</td>\n",
              "      <td>3.236005</td>\n",
              "      <td>1.525014</td>\n",
              "      <td>7.476288</td>\n",
              "      <td>4.314674</td>\n",
              "      <td>8.554956</td>\n",
              "      <td>13.204389</td>\n",
              "      <td>7.773852</td>\n",
              "      <td>130.890831</td>\n",
              "      <td>53.784638</td>\n",
              "      <td>99.311884</td>\n",
              "      <td>57.095034</td>\n",
              "      <td>76.027525</td>\n",
              "      <td>38.422912</td>\n",
              "      <td>4.351869</td>\n",
              "      <td>3.161614</td>\n",
              "      <td>1000</td>\n",
              "      <td>51.125525</td>\n",
              "      <td>58.438255</td>\n",
              "      <td>68.930434</td>\n",
              "      <td>74.717029</td>\n",
              "      <td>63.970495</td>\n",
              "      <td>59.710034</td>\n",
              "      <td>58.883378</td>\n",
              "      <td>51.761414</td>\n",
              "      <td>47.310187</td>\n",
              "      <td>81.902582</td>\n",
              "      <td>93.793717</td>\n",
              "      <td>130.103014</td>\n",
              "      <td>71.982704</td>\n",
              "      <td>36.118530</td>\n",
              "      <td>31.603714</td>\n",
              "      <td>19.648989</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 190 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   # Purchases  B01001001   B01001002  ...  B19001015  B19001016  B19001017\n",
              "0           22     206252  469.226965  ...  36.440765  23.446284  21.197485\n",
              "1            7      61399  486.538869  ...  33.000508  33.169741  24.792689\n",
              "2            3      73170  489.859232  ...  20.125056  11.890525  16.537397\n",
              "3           94     251724  505.585483  ...  43.731067  38.851729  40.427349\n",
              "4            0      37382  495.586111  ...  36.118530  31.603714  19.648989\n",
              "\n",
              "[5 rows x 190 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bxT88AjBkN0Y",
        "colab": {}
      },
      "source": [
        "allvariablenames = list(alldata.columns.values)\n",
        "listofallpredictors= allvariablenames[9:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8INVGXS7lTdL",
        "outputId": "ec6272e2-b9d4-4d60-89f2-299afeebee66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "predictors = alldata[listofallpredictors]  \n",
        "target = alldata['# Purchases']   \n",
        "predictors\n",
        "target"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       22\n",
              "1        7\n",
              "2        3\n",
              "3       94\n",
              "4        0\n",
              "      ... \n",
              "727      4\n",
              "728      3\n",
              "729      3\n",
              "730      1\n",
              "731    507\n",
              "Name: # Purchases, Length: 732, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lDv__v6jmBuT",
        "colab": {}
      },
      "source": [
        "pred_train, pred_test, tar_train, tar_test = train_test_split(predictors, target, test_size=.3, random_state=123)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ohvMrm1OmXbY",
        "outputId": "1640ea5b-37c7-4374-fda9-6fd71eb2f94c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model=LassoLarsCV(cv=10,precompute=False)\n",
        "model.fit(predictors,target)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.526e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.263e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.263e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=2.395e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.135e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.669e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 8.878e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.422e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 9.003e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.380e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 9.003e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.343e-01, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=1.277e-01, previous alpha=1.235e-01, with an active set of 45 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 168 iterations, i.e. alpha=5.721e-03, with an active set of 134 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 169 iterations, i.e. alpha=5.320e-03, with an active set of 135 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 169 iterations, i.e. alpha=5.320e-03, with an active set of 135 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 169 iterations, i.e. alpha=5.320e-03, with an active set of 135 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 169 iterations, i.e. alpha=4.884e-03, with an active set of 135 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 169 iterations, i.e. alpha=4.876e-03, with an active set of 135 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 169 iterations, i.e. alpha=4.730e-03, with an active set of 135 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 169 iterations, i.e. alpha=4.715e-03, with an active set of 135 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 169 iterations, i.e. alpha=4.710e-03, with an active set of 135 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 169 iterations, i.e. alpha=4.686e-03, with an active set of 135 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 169 iterations, i.e. alpha=4.574e-03, with an active set of 135 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 169 iterations, i.e. alpha=4.338e-03, with an active set of 135 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 170 iterations, alpha=4.910e-03, previous alpha=4.332e-03, with an active set of 135 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=1.179e-01, with an active set of 43 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=5.895e-02, with an active set of 71 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=5.895e-02, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=5.890e-02, with an active set of 71 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 89 iterations, alpha=5.854e-02, previous alpha=5.839e-02, with an active set of 74 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.512e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.756e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.643e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 22 iterations, alpha=3.141e-01, previous alpha=3.088e-01, with an active set of 21 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.790e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 8.941e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=2.830e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.690e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 9.003e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 41 iterations, alpha=1.683e-01, previous alpha=1.674e-01, with an active set of 36 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.454e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.297e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.104e+00, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.297e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.224e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.297e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.596e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=3.326e-01, with an active set of 22 regressors, and the smallest cholesky pivot element being 8.297e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=3.326e-01, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 25 iterations, alpha=3.462e-01, previous alpha=3.309e-01, with an active set of 22 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.072e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 7.955e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.310e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.036e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 7.955e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.515e-01, with an active set of 21 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.903e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.867e-01, with an active set of 29 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.568e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 44 iterations, alpha=1.528e-01, previous alpha=1.490e-01, with an active set of 39 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.761e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 7.671e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.608e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=3.881e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.184e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=2.477e-01, previous alpha=1.802e-01, with an active set of 33 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=4.169e-01, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 30 iterations, alpha=2.537e-01, previous alpha=2.486e-01, with an active set of 25 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.711e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=2.813e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.245e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.245e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 33 iterations, alpha=2.187e-01, previous alpha=2.051e-01, with an active set of 26 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.070e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.750e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.035e-01, with an active set of 18 regressors, and the smallest cholesky pivot element being 8.752e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LassoLarsCV(copy_X=True, cv=10, eps=2.220446049250313e-16, fit_intercept=True,\n",
              "            max_iter=500, max_n_alphas=1000, n_jobs=None, normalize=True,\n",
              "            positive=False, precompute=False, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dZVh21pGoSda",
        "outputId": "7192897d-6cb4-408d-ee81-1a60a69f0ed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "predictors_model=pd.DataFrame(listofallpredictors)\n",
        "predictors_model.columns = ['label']\n",
        "predictors_model['coeff'] = model.coef_\n",
        "\n",
        "for index, row in predictors_model.iterrows():\n",
        "    if row['coeff'] > 0:\n",
        "        print(row.values)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['B01001014' 0.6626591370361307]\n",
            "['B01001036' 1.7477634015843677]\n",
            "['B01001037' 2.2471543892392045]\n",
            "['B01001038' 2.032958095569556]\n",
            "['B02001005' 1.0444093395536234]\n",
            "['B02001007' 0.0026501086395314334]\n",
            "['B13014026' 0.6378886301460416]\n",
            "['B13014027' 1.373831154949208]\n",
            "['B13016008' 0.7544555704553156]\n",
            "['B15002027' 4.107586791367716]\n",
            "['B19001017' 1.8782666975455609]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Sm61n7nHpN2l"
      },
      "source": [
        "Question 1. The code above is running a four loop. First we define some variables. The first line of code is defining a variable predictors_model that creates a data frame of the predictor variables and coeficents. The second line is putting the predictors as in under'label'. The next line is putting coefs from the model in predictors_model['coeff'] column. Next is the for loop. The first line is saying for every row in predictor_model iterate through each row and do the following; if the coeff of that row is greater than 0 print the row and the value. Its printing the label and coefs that have a coef greater than 0. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kZGVGN7TrhWK"
      },
      "source": [
        "Question 2. In areas where there are more females over 25 with no diploma we sell more bobo bars. In areas with females aged 35 to 39 we sell more bobo bars. In aeas with females aged 40 to 44 we sell more. Areas with households that make $200000 or more are selling mroe bobo bars.Areas with women 30 to 34 sell more bobo bars. Areas with women 15 to 50 years old, who did not have a brith in the past 12 months, and never married or is widowed or divorced and has a graduate or professional degree we sell more bobo bars. Areas with more asians sell more bobo bars. Areas with males 40 to 44 sell more bobo bars. Areas with women who had a child in the last year and are 40 to 44 sell more bobos. Areas with women aged 15 to 50 who has a birth in the last year with a bachelors degree sell more bobo bars. Areas with people reporting other race sell more bobos. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RTVhMGK9ulv0"
      },
      "source": [
        "Questin 3. The top two best predictor variables for sales are women 25 year and over -12th grade, no diploma. The second varibles is females aged 35 to 39."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vM2KoTV3sboM",
        "outputId": "a1c8ced3-0eeb-4a3d-d7f6-5343191e847b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_error = mean_squared_error(tar_train, model.predict(pred_train))\n",
        "print ('training data MSE')\n",
        "print(train_error)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data MSE\n",
            "20955.59181858268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0KJyr9EZwD6o",
        "outputId": "dcb17c6b-0820-485c-9b09-ab09ca039650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "test_error = mean_squared_error(tar_test, model.predict(pred_test))\n",
        "print ('test data MSE')\n",
        "print(test_error)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test data MSE\n",
            "36219.3364898267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LERCfQlrw_cI"
      },
      "source": [
        "The training and test set mean standard errors are different, test data is higher than training data error. This means the test data seems to be further off from the actual data becasue it has a larger MSE. The training data is still off from the actual data but not as bad as the test data. This could indicate there is more vairability in the test data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rAzBPpeKxllD",
        "outputId": "51b0dc1e-80e7-413c-edd0-e87c07abf479",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "rsquared_train=model.score(pred_train,tar_train)\n",
        "print ('training data R-square')\n",
        "print(rsquared_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data R-square\n",
            "0.27693842762902243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ujNeAOyWxpAh"
      },
      "source": [
        "question 5. We can say that census data overall can help predict 27.7% of sales. The r- squared value measures the propotion of variance for a dependent variable explained by an independent variable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mRQ3_-jJyQG7",
        "outputId": "df5f41fd-524e-47cb-ef6e-2e6e3babd55e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"y interecept:\")\n",
        "print(model.intercept_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y interecept:\n",
            "30.487098362376265\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XUpzfURKyWPh"
      },
      "source": [
        "the y intercept represents the sales when the predictors are 0. in this cases it would be the sales given no predictor variables or no information on the demogrphics or census.  "
      ]
    }
  ]
}